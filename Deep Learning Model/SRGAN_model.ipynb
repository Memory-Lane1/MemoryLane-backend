{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SRGAN_model.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iBOwVbnKj6WC",
        "outputId": "09242f76-4af2-415b-acc2-d96a78701924"
      },
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow.keras as keras\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import cv2\n",
        "from PIL import Image\n",
        "import os\n",
        "print(tf.__version__)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.6.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R6e-78bcj_Qj"
      },
      "source": [
        "def discriminator_model():\n",
        "  input_layer=keras.Input(shape=(720,1024,3))\n",
        "  x=keras.layers.Conv2D(32,(11,11),strides=2)(input_layer)\n",
        "  x=keras.layers.LeakyReLU()(x)\n",
        "  x=keras.layers.Conv2D(64,(7,7),strides=2,kernel_regularizer=keras.regularizers.l2)(x)\n",
        "  x=keras.layers.BatchNormalization()(x)\n",
        "  x=keras.layers.LeakyReLU()(x)\n",
        "  x=keras.layers.Conv2D(64,(5,5),kernel_regularizer=keras.regularizers.l2,strides=2)(x)\n",
        "  x=keras.layers.BatchNormalization()(x)\n",
        "  x=keras.layers.LeakyReLU()(x)\n",
        "  x=keras.layers.Conv2D(128,(3,3),kernel_regularizer=keras.regularizers.l2,strides=2)(x)\n",
        "  x=keras.layers.BatchNormalization()(x)\n",
        "  x=keras.layers.LeakyReLU()(x)\n",
        "  x=keras.layers.Conv2D(128,(3,3),kernel_regularizer=keras.regularizers.l2,strides=2)(x)\n",
        "  x=keras.layers.BatchNormalization()(x)\n",
        "  x=keras.layers.LeakyReLU()(x)\n",
        "  x=keras.layers.Conv2D(128,(3,3),kernel_regularizer=keras.regularizers.l2,strides=2)(x)\n",
        "  x=keras.layers.BatchNormalization()(x)\n",
        "  x=keras.layers.LeakyReLU()(x)\n",
        "  x=keras.layers.Conv2D(128,(3,3),kernel_regularizer=keras.regularizers.l2,strides=1)(x)\n",
        "  x=keras.layers.BatchNormalization()(x)\n",
        "  x=keras.layers.LeakyReLU()(x)\n",
        "  x=keras.layers.Flatten()(x)\n",
        "  x=keras.layers.Dense(1024,kernel_regularizer=keras.regularizers.l2,)(x)\n",
        "  x=keras.layers.LeakyReLU()(x)\n",
        "  x=keras.layers.Dense(256,kernel_regularizer=keras.regularizers.l2,)(x)\n",
        "  x=keras.layers.LeakyReLU()(x)\n",
        "  x=keras.layers.Dense(1,activation='sigmoid',kernel_regularizer=keras.regularizers.l2)(x)\n",
        "  model=keras.Model(inputs=input_layer,outputs=x)\n",
        "  return model\n",
        "\n",
        "def generator_block(i,filters):\n",
        "    input_layer=keras.Input(shape=i.shape[1:])\n",
        "    o=keras.layers.Conv2D(filters,(3,3),padding='same',kernel_regularizer=keras.regularizers.l2)(input_layer)\n",
        "    o=keras.layers.BatchNormalization()(o)\n",
        "    o=keras.layers.LeakyReLU()(o)\n",
        "    o=keras.layers.Conv2D(filters,(3,3),padding='same',kernel_regularizer=keras.regularizers.l2)(o)\n",
        "    o=keras.layers.BatchNormalization()(o)\n",
        "    o=keras.layers.add([input_layer,o])\n",
        "    model_block=keras.Model(inputs=input_layer,outputs=o)\n",
        "    return model_block\n",
        "\n",
        "def generator_model():\n",
        "  inputs=keras.Input(shape=(180,256,3))\n",
        "  x=keras.layers.Conv2D(32,(5,5),padding='same')(inputs)\n",
        "  x=keras.layers.LeakyReLU()(x)\n",
        "  skip=x\n",
        "  x=generator_block(x,32)(x)\n",
        "  x=generator_block(x,32)(x)\n",
        "  x=keras.layers.Conv2D(64,(5,5),padding='same')(x)\n",
        "  x=keras.layers.LeakyReLU()(x)\n",
        "  x=generator_block(x,64)(x)\n",
        "  x=generator_block(x,64)(x)\n",
        "  x=keras.layers.Conv2D(64,(5,5),padding='same')(x)\n",
        "  x=keras.layers.LeakyReLU()(x)\n",
        "  x=generator_block(x,64)(x)\n",
        "  x=generator_block(x,64)(x)\n",
        "  x=keras.layers.Conv2D(64,(5,5),padding='same')(x)\n",
        "  x=keras.layers.LeakyReLU()(x)\n",
        "  x=generator_block(x,64)(x)\n",
        "  x=generator_block(x,64)(x)\n",
        "  x=keras.layers.Conv2D(32,(3,3),padding='same')(x)\n",
        "  x=keras.layers.LeakyReLU()(x)\n",
        "  x=keras.layers.BatchNormalization()(x)\n",
        "  x=keras.layers.add([x,skip])\n",
        "  x=keras.layers.Conv2DTranspose(32,(2,2),strides=4)(x)\n",
        "  x=keras.layers.Conv2D(32,(3,3),padding='same')(x)\n",
        "  x=keras.layers.LeakyReLU()(x)\n",
        "  x=generator_block(x,32)(x)\n",
        "  x=keras.layers.Conv2D(3,(3,3),padding='same',activation=keras.activations.sigmoid)(x)\n",
        "  generator=keras.Model(inputs=inputs,outputs=x)\n",
        "  return generator"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jwamx0yRkKbS"
      },
      "source": [
        "class SRGAN(keras.Model):\n",
        "  def __init__(self,discriminator,generator):\n",
        "    super(SRGAN, self).__init__()\n",
        "    self.discriminator=discriminator\n",
        "    self.generator=generator\n",
        "  def compile(self, loss_fn,discriminator_op,generator_op):\n",
        "    super(SRGAN, self).compile()\n",
        "    self.loss_fn=loss_fn\n",
        "    self.discriminator_op=discriminator_op\n",
        "    self.generator_op=generator_op\n",
        "  def train_step(self,data):\n",
        "    input_images=data[0]\n",
        "    target_images=data[1]\n",
        "    batch_size=1\n",
        "    print(input_images.shape)\n",
        "    for k in range(1):\n",
        "      ind=np.random.randint(low=0,high=16,size=batch_size)\n",
        "      batch_real=tf.gather(target_images,ind)\n",
        "      batch_fake=self.generator(tf.gather(input_images,ind))\n",
        "      y_batch_real=np.ones((batch_size,1),dtype=np.float32)\n",
        "      y_batch_fake=np.zeros((batch_size,1),dtype=np.float32)\n",
        "      xtrain=tf.concat([batch_real,batch_fake],axis=0)\n",
        "      ytrain=tf.concat([y_batch_real,y_batch_fake],axis=0)\n",
        "      with tf.GradientTape() as tape:\n",
        "        predictions=self.discriminator(xtrain)\n",
        "        d_loss=self.loss_fn(ytrain,predictions)\n",
        "      \n",
        "      grads=tape.gradient(d_loss,self.discriminator.trainable_weights)\n",
        "      self.discriminator_op.apply_gradients(\n",
        "          zip(grads,self.discriminator.trainable_weights)\n",
        "      )\n",
        "    for k in range(5):\n",
        "      misleading_labels=np.ones((batch_size,1),dtype=np.float32)\n",
        "      ind=np.random.randint(low=0,high=16,size=batch_size)\n",
        "      batch_fake=tf.gather(input_images,ind)\n",
        "      target_sample=tf.gather(target_images,ind)\n",
        "      with tf.GradientTape() as tape:\n",
        "        generated_image=self.generator(batch_fake)\n",
        "        vgg_1_real=vgg_model_1(target_sample)\n",
        "        vgg_1_fake=vgg_model_1(generated_image)\n",
        "        vgg_2_real=vgg_model_2(target_sample)\n",
        "        vgg_2_fake=vgg_model_2(generated_image)\n",
        "        predictions=self.discriminator(generated_image)\n",
        "        g_loss=0.001*self.loss_fn(misleading_labels,predictions) \n",
        "        g_loss+=0.5*tf.keras.losses.MeanSquaredError()(vgg_2_real,vgg_2_fake)\n",
        "        g_loss+=tf.keras.losses.MeanSquaredError()(vgg_1_real,vgg_1_fake)\n",
        "        g_loss+=tf.keras.losses.MeanSquaredError()(self.generator(batch_fake),target_sample)\n",
        "      grads=tape.gradient(g_loss,self.generator.trainable_weights)\n",
        "      self.generator_op.apply_gradients(\n",
        "          zip(grads,self.generator.trainable_weights)\n",
        "      )\n",
        "\n",
        "    return {'d_loss':d_loss,'g_loss':g_loss}\n",
        "\n",
        "\n",
        "  def get_config(self):\n",
        "    config = super(Linear, self).get_config()\n",
        "    config.update({\"units\": self.units})\n",
        "    return config\n",
        "\n",
        "dis=discriminator_model()\n",
        "gen=generator_model()\n",
        "\n",
        "gan=SRGAN(dis,gen)\n",
        "gan.compile(\n",
        "    discriminator_op=keras.optimizers.Adam(learning_rate=0.0003),\n",
        "    generator_op=keras.optimizers.Adam(learning_rate=0.0003),\n",
        "    loss_fn=keras.losses.BinaryCrossentropy()\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VQj0vsH5kK_i"
      },
      "source": [
        "# dis.load_weights('/content/drive/MyDrive//dis_weight_final.h5')\n",
        "gen.load_weights('/content/drive/MyDrive/MemoryLane/gen_weight_final.h5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N9lQCqN-k2dc"
      },
      "source": [
        "def showImages(path_to_img,offset=0,crop=True):\n",
        "  img=Image.open(path_to_img)\n",
        "  i=offset\n",
        "  if crop:\n",
        "    img = np.asarray( img, dtype=\"int32\" )[i:180+i,i:256+i,:]\n",
        "  else:\n",
        "    img = np.asarray( img, dtype=\"int32\" )[:360:2,:512:2,:]\n",
        "  # img = np.asarray( img, dtype=\"int32\" )[i:180+i,i:256+i,:]\n",
        "\n",
        "  gen_img=gen.predict(img.reshape(1,180,256,3)/255.)\n",
        "  fig,ax=plt.subplots(1,2,figsize=(40, 20),gridspec_kw={'wspace': 0.03})\n",
        "  ax[0].imshow(img)\n",
        "  ax[0].axis('off')\n",
        "  ax[1].imshow(gen_img[0])\n",
        "  ax[1].axis('off')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h5W4Ws_mAW2X",
        "outputId": "82d4ab49-fa70-433f-bd73-7ef7ef28bc31"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JcZwrfGglIcy"
      },
      "source": [
        "img_names=[i for i in os.listdir('/content/drive/MyDrive/MemoryLane/Dataset_test')]\n",
        "target_paths=['/content/drive/MyDrive/MemoryLane/Dataset_test/'+i for i in img_names]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "woFs9vSQqNTc"
      },
      "source": [
        "# showImages(target_paths[1],0)"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TKG7pHDPqQfn"
      },
      "source": [
        "# 54, '/content/drive/MyDrive/SRGAN/aaaaa.jpg' , 76[50:] , 97[50:] , 654 , 578 ,392[0:] ,322[0:] , 473[50:]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AO6aGOe8DTEU"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}